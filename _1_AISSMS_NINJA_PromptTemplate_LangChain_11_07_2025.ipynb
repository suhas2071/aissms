{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  langchain langchain_openai tiktoken langchain-community langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXRZzjib9ZO4",
        "outputId": "846ac8c0-fcd5-4f4c-d4bc-a6cdb115809f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhHjLHnaf0M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "xI8Tm8QUzkiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RWitWUB5_xbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt_template = \"What is a good name for a company that makes {product}?\"\n",
        "\n",
        "llm = OpenAI(temperature=0.2)\n",
        "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
        "llm_chain(\"Shoes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJWhAaxOL8Zo",
        "outputId": "e48a5e9c-80b6-4bf5-c157-37c8ed95acc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-699965269.py:7: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  llm = OpenAI(temperature=0.2)\n",
            "/tmp/ipython-input-3-699965269.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
            "/tmp/ipython-input-3-699965269.py:9: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  llm_chain(\"Shoes\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product': 'Shoes', 'text': '\\n\\nSoleCraft Co.'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt_template = \"What is a good name for a company that makes {product}?\"\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
        "llm_chain(\"shoes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yiUEbtL9u-r",
        "outputId": "a134ba2e-f3c4-4dfc-9cb3-5477cafc79bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product': 'shoes', 'text': 'SoleStride Footwear Co.'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage,ToolMessage\n",
        "\n",
        "# It's highly recommended to use model_name=\"gpt-3.5-turbo\" or \"gpt-4\" or \"gpt-4o\"\n",
        "chat_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7) # Recommended model for general use\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"What is the capital of France?\"),\n",
        "]\n",
        "\n",
        "response = chat_llm.invoke(messages)\n",
        "print(response.content)\n",
        "\n",
        "# Example with a follow-up\n",
        "messages.append(AIMessage(content=response.content)) # Add AI's response to history\n",
        "messages.append(HumanMessage(content=\"And what is its most famous landmark?\"))\n",
        "\n",
        "follow_up_response = chat_llm.invoke(messages)\n",
        "print(follow_up_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce5ye5Of9OIC",
        "outputId": "9fd03436-06c5-4aea-94e8-a89cc0027ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n",
            "The most famous landmark in Paris is the Eiffel Tower.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 1. Define the AI's role and instructions.\n",
        "prompt_template = \"\"\"\n",
        "You are a friendly car mechanic. Answer the question below in simple terms.\n",
        "Do not use technical words, give easy/\n",
        "to understand responses in bullet format\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# 2. Create a prompt using the template.\n",
        "prompt = PromptTemplate.from_template(template=prompt_template)\n",
        "\n",
        "# 3. Fill in the question.\n",
        "formatted_prompt = prompt.format(question=\"Why won't my car start?\")\n",
        "\n",
        "# 4. Connect to OpenAI.\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "# 5. Ask the AI the question and get the answer.\n",
        "answer = llm.predict(formatted_prompt)\n",
        "\n",
        "# 6. Show the answer.\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L8IUlZhie9h",
        "outputId": "773902c3-2504-4fe7-e50d-53b19939629b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-945865199.py:22: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  answer = llm.predict(formatted_prompt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- There could be a problem with the battery.\n",
            "- The fuel tank may be empty.\n",
            "- The spark plugs could be worn out or dirty.\n",
            "- The starter or ignition switch may be faulty.\n",
            "- There could be an issue with the fuel pump.\n",
            "- The engine may have overheated.\n",
            "- There could be a problem with the electrical system.\n",
            "- The car may be out of gas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, ensure you have langchain-openai installed:\n",
        "# pip install langchain-openai\n",
        "\n",
        "# Import the ChatOpenAI model\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Import the specific message types and chat prompt template\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "\n",
        "# 1. Define the AI's role and instructions using SystemMessage.\n",
        "#    You create a list of messages.\n",
        "#    The first message is typically a SystemMessage to set the persona.\n",
        "#    Then, a HumanMessage for the actual user query.\n",
        "\n",
        "# Instead of PromptTemplate.from_template, you often build a list of messages directly\n",
        "# or use ChatPromptTemplate for more complex prompt engineering.\n",
        "# For this simple case, we'll build the list of messages directly.\n",
        "\n",
        "# System message to set the persona and tone\n",
        "system_message_content = \"\"\"\n",
        "You are a friendly car mechanic. Answer the question below in simple terms.\n",
        "Do not use technical words, give easy-to-understand responses in bullet format.\n",
        "\"\"\"\n",
        "system_message = SystemMessage(content=system_message_content.strip())\n",
        "\n",
        "\n",
        "# Human message for the user's question\n",
        "human_question = HumanMessage(content=\"Why won't my car start?\")\n",
        "\n",
        "\n",
        "# Combine them into a list of messages\n",
        "messages_for_llm = [\n",
        "    system_message,\n",
        "    human_question\n",
        "]\n",
        "\n",
        "# 2. Ask the AI the question and get the answer.\n",
        "#    Use .invoke() for a single turn, or .stream() for streaming responses.\n",
        "\n",
        "# *** FIX: Explicitly instantiate ChatOpenAI here ***\n",
        "# Use a different variable name or reassign llm if you intend to use ChatOpenAI\n",
        "chat_llm_instance = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7) # Using a different variable name to avoid confusion\n",
        "\n",
        "print(\"--- Sending message to LLM ---\")\n",
        "# *** FIX: Use the ChatOpenAI instance's invoke method ***\n",
        "answer_message = chat_llm_instance.invoke(messages_for_llm)\n",
        "\n",
        "# 3. Show the answer.\n",
        "#    The response from ChatOpenAI's invoke is an AIMessage object,\n",
        "#    so you access its content attribute.\n",
        "print(\"\\n--- AI's Answer ---\")\n",
        "print(answer_message.content) # This will now work as answer_message is an AIMessage object\n",
        "\n",
        "# --- Example of a follow-up turn (showing memory concept in action) ---\n",
        "# In a real conversation, you'd save the history.\n",
        "messages_for_llm.append(answer_message) # Add AI's previous answer\n",
        "follow_up_human_message = HumanMessage(content=\"What should I check first for a dead battery?\")\n",
        "messages_for_llm.append(follow_up_human_message)\n",
        "# *** FIX: Use the ChatOpenAI instance's invoke method for the follow-up ***\n",
        "follow_up_answer_message = chat_llm_instance.invoke(messages_for_llm)\n",
        "print(\"\\n--- Follow-up Answer ---\")\n",
        "print(follow_up_answer_message.content) # This will also work"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG9_FatsAnoj",
        "outputId": "a2f56460-33cf-477b-cf2b-c6ea7aeb110b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sending message to LLM ---\n",
            "\n",
            "--- AI's Answer ---\n",
            "Here are some simple reasons why your car might not start:\n",
            "\n",
            "- **Dead Battery**: The battery might be out of power.\n",
            "- **Bad Starter**: The part that helps start the engine might be broken.\n",
            "- **Empty Gas Tank**: You might not have enough fuel in the tank.\n",
            "- **Fuel Issues**: There could be a problem with the fuel getting to the engine.\n",
            "- **Faulty Key**: The key might not be working properly.\n",
            "- **Electrical Problems**: There could be an issue with the car's wiring or fuses.\n",
            "- **Bad Alternator**: The part that charges the battery while you drive might not be working.\n",
            "\n",
            "If your car won't start, it might be good to get it checked by a mechanic!\n",
            "\n",
            "--- Follow-up Answer ---\n",
            "If you think your battery might be dead, here are some simple things to check first:\n",
            "\n",
            "- **Lights**: Turn on your car's lights. If they are very dim or don't come on, the battery could be dead.\n",
            "- **Sounds**: Listen for a clicking sound when you turn the key. This can mean the battery doesn’t have enough power.\n",
            "- **Connections**: Check the battery connections. Make sure the cables are tight and not corroded (look for white stuff around the connections).\n",
            "- **Age of the Battery**: If your battery is older than 3-5 years, it might just be time for a new one.\n",
            "\n",
            "If you find something wrong, you might need a jump start or a new battery!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://python.langchain.com/docs/modules/chains/foundational/llm_chain"
      ],
      "metadata": {
        "id": "o-Sl7bZqMpEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD1fg82ANzIS",
        "outputId": "bc17da0a-df7e-44b0-c488-4f743cd4dbb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Dead battery: The most common reason for a vehicle not starting is a dead battery. This can happen if you accidentally left your lights on or if the battery is old and needs to be replaced.\n",
            "\n",
            "2. Fuel issue: If your vehicle has trouble starting, it could be due to a lack of fuel. Make sure your gas tank is not empty and that your fuel pump is functioning properly.\n",
            "\n",
            "3. Faulty starter: The starter is responsible for turning the engine over and if it is not working correctly, your vehicle won't start. This can be caused by a worn out starter or a faulty ignition switch.\n",
            "\n",
            "4. Bad spark plugs: Spark plugs are essential for igniting the fuel in your engine. If they are worn out or dirty, your vehicle may have trouble starting.\n",
            "\n",
            "5. Clogged fuel filter: A clogged fuel filter can prevent fuel from reaching the engine, causing your vehicle to have trouble starting.\n",
            "\n",
            "6. Ignition system issues: The ignition system includes components like the spark plugs, ignition coil, and distributor. If any of these parts are faulty, your vehicle may not start.\n",
            "\n",
            "7. Electrical problems: A faulty electrical system can also prevent your vehicle from starting. This can be caused by a blown fuse, a faulty relay, or a\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "# import openAI from langChain\n",
        "\n",
        "# import prompt template\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# create the prompt\n",
        "prompt_template: str = \"\"\"/\n",
        "You are a vehicle mechanic, give responses to the following/\n",
        "question: {question}. Do not use technical words, give easy/\n",
        "to understand responses.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template=prompt_template)\n",
        "\n",
        "# format the prompt to add variable values\n",
        "prompt_formatted_str: str = prompt.format(\n",
        "    question=\"Why won't a vehicle start on ignition?\")\n",
        "\n",
        "# instantiate the OpenAI intance\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "# make a prediction\n",
        "prediction = llm.predict(prompt_formatted_str)\n",
        "\n",
        "# print the prediction\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "# import openAI from langChain\n",
        "\n",
        "# import prompt template\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# create the prompt\n",
        "prompt_template: str = \"\"\"/\n",
        "You are a Doctor, give responses to the following/\n",
        "question: {question}. Do not use complex medical words, give easy/\n",
        "to understand responses.Do not use technical words, give easy/\n",
        "to understand responses in bullet format\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template=prompt_template)\n",
        "\n",
        "# format the prompt to add variable values\n",
        "prompt_formatted_str: str = prompt.format(\n",
        "    question=\"How to control Diabetes without taking medicine?\")\n",
        "\n",
        "# instantiate the OpenAI intance\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "# make a prediction\n",
        "prediction = llm.predict(prompt_formatted_str)\n",
        "\n",
        "# print the prediction\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mShkkAheV6Yq",
        "outputId": "f06ab095-3b6e-488c-fc18-a63977c7ee6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- Regular exercise can help control diabetes by improving insulin sensitivity and reducing blood sugar levels.\n",
            "- Eating a healthy and balanced diet, including plenty of fruits and vegetables, whole grains, and lean protein can also help manage diabetes.\n",
            "- Monitoring blood sugar levels and making necessary adjustments to diet and medication can help keep diabetes under control.\n",
            "- Managing stress levels and getting enough sleep can also play a role in controlling diabetes.\n",
            "- Quitting smoking and limiting alcohol consumption can also improve diabetes management.\n",
            "- Some natural supplements, such as cinnamon, may also have a positive impact on blood sugar levels.\n",
            "- Working closely with a healthcare team, including a doctor and registered dietitian, can provide personalized guidance and support for managing diabetes without medication.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# 1. Create a simple instruction for the AI.\n",
        "instruction = \"\"\"\n",
        "You are a friendly doctor. Please answer the question below using simple words.\n",
        "Do not use technical words, give easy/\n",
        "to understand responses in bullet format\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# 2. Make a prompt from the instruction.\n",
        "prompt = PromptTemplate.from_template(template=instruction)\n",
        "\n",
        "# 3. Put the question into the prompt.\n",
        "my_question = \"How to control Diabetes without taking medicine?\"\n",
        "ready_prompt = prompt.format(question=my_question)\n",
        "\n",
        "# 4. Connect to the AI.\n",
        "ai = OpenAI(temperature=0.7)\n",
        "\n",
        "# 5. Ask the AI and get the answer.\n",
        "answer = ai.predict(ready_prompt)\n",
        "\n",
        "# 6. Show the answer.\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPAuQ-v3jQQU",
        "outputId": "e8e2002b-7903-4454-fe2d-9e1828dd0bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- Eat a healthy and balanced diet\n",
            "- Exercise regularly\n",
            "- Maintain a healthy weight\n",
            "- Limit sugar and high-carb foods\n",
            "- Stay hydrated\n",
            "- Get enough sleep\n",
            "- Reduce stress levels\n",
            "- Monitor blood sugar levels regularly\n",
            "- Follow a consistent meal schedule\n",
            "- Increase fiber intake\n",
            "- Include more fruits and vegetables in your diet\n",
            "- Consult with a registered dietitian for personalized dietary recommendations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "# import openAI from langChain\n",
        "\n",
        "# import prompt template\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# create the prompt\n",
        "prompt_template: str = \"\"\"/\n",
        "You are a AWS Solution Architect, give responses to the following/\n",
        "question: {question}. Do not use complex words, give easy/\n",
        "to understand responses.Do not use technical words, give easy/\n",
        "to understand responses in bullet format\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template=prompt_template)\n",
        "\n",
        "# format the prompt to add variable values\n",
        "prompt_formatted_str: str = prompt.format(\n",
        "    question=\" You are an expert AWS Solution architect. Company wants to move 5 PB of data from on premise servers to AWS Cloud Environment. Company is not having any direct connection to transfer data. I need device you can send to our company. we will transfer 5PB of data to device and later we can upload to nearest region. Our company is based in Bengaluru location. after transfering 5PB of data to aws S3 service. 4PB of data should be kept for a period of 5- 7years. provide me cost effective solution for the same.?\")\n",
        "\n",
        "# instantiate the OpenAI intance\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "# make a prediction\n",
        "prediction = llm.predict(prompt_formatted_str)\n",
        "\n",
        "# print the prediction\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUna2DiJKfpg",
        "outputId": "0a1536f5-d4e7-4792-ddd1-589bdaa1f9fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- The most efficient and cost-effective solution would be to use AWS Snowball Edge devices.\n",
            "- These devices can be shipped to your company's location in Bengaluru and can transfer up to 100TB of data per device.\n",
            "- You can transfer the 5PB of data to the Snowball Edge devices and then ship them to the nearest AWS region for uploading to S3.\n",
            "- The remaining 4PB of data can be stored in S3 Glacier, which is a low-cost storage option for data that needs to be retained for a longer period of time.\n",
            "- With S3 Glacier, you only pay for the storage and retrieval of data, making it a cost-effective solution for long-term storage.\n",
            "- You can also set a lifecycle policy on the S3 bucket to automatically transition the data from S3 to S3 Glacier after a certain period of time.\n",
            "- Additionally, you can use AWS DataSync to continuously transfer new and updated data from your on-premises servers to S3, ensuring that your data is always up to date in the cloud. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# 1. Define the AI's role and instructions.\n",
        "instruction = \"\"\"\n",
        "You are an AWS expert. Answer the question below in simple terms, using bullet points.\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# 2. Create the prompt.\n",
        "prompt = PromptTemplate.from_template(template=instruction)\n",
        "\n",
        "# 3. Formulate the specific question.\n",
        "company_question = \"\"\"\n",
        "Our company, based in Bengaluru,\n",
        "needs to move 5 PB of data from our\n",
        "local servers to AWS. We don't have a direct connection for data transfer.\n",
        "We need a device we can use to transfer the data and then upload it to AWS.\n",
        "After uploading to AWS S3, 4 PB of data should be stored for 5-7 years.\n",
        "What's a cost-effective solution?\n",
        "\"\"\"\n",
        "\n",
        "# 4. Create the full prompt by inserting the question.\n",
        "full_prompt = prompt.format(question=company_question)\n",
        "\n",
        "# 5. Connect to the AI.\n",
        "aws_expert = OpenAI(temperature=0.7)\n",
        "\n",
        "# 6. Get the AI's answer.\n",
        "solution = aws_expert.predict(full_prompt)\n",
        "\n",
        "# 7. Show the answer.\n",
        "print(solution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFWMXu6Mtiva",
        "outputId": "a626b962-0a5b-464e-9e68-592cab60388e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "• AWS Snowball is a cost-effective solution for transferring large amounts of data (in this case, 5 PB) from local servers to AWS.\n",
            "\n",
            "• Snowball is a physical device that can be shipped to your location and used to transfer data.\n",
            "\n",
            "• Once the data is transferred to the Snowball device, it can be shipped back to AWS and the data can be uploaded to S3.\n",
            "\n",
            "• Snowball is a one-time cost, with no additional fees for data transfer.\n",
            "\n",
            "• To store 4 PB of data for 5-7 years on AWS S3, the cost will be approximately $0.023 per GB per month.\n",
            "\n",
            "• This solution is cost-effective because it eliminates the need for a direct connection for data transfer, which can be expensive.\n",
            "\n",
            "• Additionally, using Snowball reduces the time and resources needed for data transfer, making it a more efficient and cost-effective option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ5pieeauHYZ",
        "outputId": "2e0f04c7-1093-4156-f9a0-b3cfb3bc5762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.14)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "def generate_aws_solution(user_question):\n",
        "    \"\"\"Generates an AWS solution based on the user's question.\"\"\"\n",
        "\n",
        "    # 1. Define the AI's role and instructions.\n",
        "    instruction = \"\"\"\n",
        "    You are an AWS expert. Answer the question below in simple terms, using bullet points.\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "\n",
        "    # 2. Create the prompt.\n",
        "    prompt = PromptTemplate.from_template(template=instruction)\n",
        "\n",
        "    # 3. Create the full prompt by inserting the question.\n",
        "    full_prompt = prompt.format(question=user_question)\n",
        "\n",
        "    # 4. Connect to the AI.\n",
        "    aws_expert = OpenAI(temperature=0.7)\n",
        "\n",
        "    # 5. Get the AI's answer.\n",
        "    solution = aws_expert.predict(full_prompt)\n",
        "\n",
        "    return solution\n",
        "\n",
        "# Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_aws_solution,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Enter your AWS question here...\"),\n",
        "    outputs=gr.Textbox(lines=10, placeholder=\"AWS Solution will appear here...\"),\n",
        "    title=\"AWS Solution Architect Chatbot\",\n",
        "    description=\"Ask your AWS-related questions, and the AI will provide a simple, bullet-point solution.\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "4ihHmaN3uCHw",
        "outputId": "1bd6e4fe-be23-4cae-bdb9-b6b12d57fb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ea28d952bda6400a59.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ea28d952bda6400a59.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}